{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence Classificaton.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwLvxfyBr7sK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0987b267-4d4b-4b99-c56a-004f5042ff78"
      },
      "source": [
        "#import all the necessary libraries required\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgMRTUJCvHTq"
      },
      "source": [
        "#reading training and testing datasets using pandas\r\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/AG Sentence Classification Task/train.csv\" , header = None) \r\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/AG Sentence Classification Task/test.csv\" , header = None)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_06L1vD6wDk-"
      },
      "source": [
        "#mapping labels to their respective classes\r\n",
        "labels_to_class_mapping = {1 : \"World\" , 2 : \"Sports\" ,3 : \"Business\" , 4 : \"Sci/Tech\"}"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hun90xBp0EYQ"
      },
      "source": [
        "\r\n",
        "#function to map labels\r\n",
        "def labels_mapping(l):\r\n",
        "  classes = []\r\n",
        "  for i in range(0 , len(l)):\r\n",
        "    classes.append(labels_to_class_mapping[l[i]])\r\n",
        "  return classes\r\n",
        "#initializing tokenizer\r\n",
        "tokenizer=RegexpTokenizer(r'\\w+')\r\n",
        "\r\n",
        "#Getting stop words\r\n",
        "en_stopwords=set(stopwords.words('english'))\r\n",
        "\r\n",
        "#initializing stemming Object\r\n",
        "ps=PorterStemmer()\r\n",
        "\r\n",
        "#function to clean data using stemming algorithm\r\n",
        "def getCleanarticle(article):\r\n",
        "    #Tokenize\r\n",
        "    tokens=tokenizer.tokenize(article)\r\n",
        "    new_tokens=[token for token in tokens if token not in  en_stopwords]\r\n",
        "    stemmed_tokens=[ps.stem(token) for token in new_tokens]\r\n",
        "    clean_article=' '.join(stemmed_tokens)\r\n",
        "    return clean_article"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpda2xeGplk-"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer \r\n",
        "#function to clean data using lemmatization algorithm\r\n",
        "lemmatizer = WordNetLemmatizer() \r\n",
        "def cleanreview_lemmatize(article):\r\n",
        "   tokens=tokenizer.tokenize(article)\r\n",
        "   new_tokens=[token for token in tokens if token not in  en_stopwords]\r\n",
        "   lemmatize_tokens=[lemmatizer.lemmatize(token) for token in new_tokens]\r\n",
        "   clean_article=' '.join(lemmatize_tokens)\r\n",
        "   return clean_article"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4XweJld3R8n",
        "outputId": "9b754d22-b591-4340-e832-4cdcead03fb4"
      },
      "source": [
        "# df[2].apply(getCleanarticle)\r\n",
        "# df_test[2].apply(getCleanarticle)\r\n",
        "\r\n",
        "nltk.download('wordnet')\r\n",
        "#cleaning training and testing datasets\r\n",
        "df[2].apply(cleanreview_lemmatize)\r\n",
        "df_test[2].apply(cleanreview_lemmatize)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Unions representing worker Turner Newall say d...\n",
              "1       SPACE com TORONTO Canada A second team rockete...\n",
              "2       AP A company founded chemistry researcher Univ...\n",
              "3       AP It barely dawn Mike Fitzpatrick start shift...\n",
              "4       AP Southern California smog fighting agency we...\n",
              "                              ...                        \n",
              "7595    Ukrainian presidential candidate Viktor Yushch...\n",
              "7596    With supply attractive pitching option dwindli...\n",
              "7597    Like Roger Clemens almost exactly eight year e...\n",
              "7598    SINGAPORE Doctors United States warned painkil...\n",
              "7599    EBay plan buy apartment home rental service Re...\n",
              "Name: 2, Length: 7600, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAepdmJK5mS1"
      },
      "source": [
        "#splitting training and testing dataset\r\n",
        "X_train =df.loc[:, 2].values\r\n",
        "y_train =df.loc[:, 0].values\r\n",
        "X_test =df_test.loc[:, 2].values\r\n",
        "y_test =df_test.loc[:, 0].values"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEUcclhDk7Qe"
      },
      "source": [
        "y_train = labels_mapping(y_train)\r\n",
        "y_test = labels_mapping(y_test)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRHZw7IY7LNC"
      },
      "source": [
        "labels = [\"World\", \"Sports\" ,\"Business\" ,\"Sci/Tech\"]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0Koj0N8D4lh",
        "outputId": "7cf30125-3b8d-47d1-a82f-c7f6295e2543"
      },
      "source": [
        "#initializing vectorization \r\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True, encoding='utf-8',\r\n",
        " decode_error='ignore')\r\n",
        "#Creating NLP pipeline model using Multinomial Naive Bayes Classification Algorithm\r\n",
        "nb = Pipeline([('vect', vectorizer),\r\n",
        "               ('tfidf', TfidfTransformer()),\r\n",
        "               ('clf', MultinomialNB()),\r\n",
        "              ])\r\n",
        "nb.fit(X_train, y_train)\r\n",
        "\r\n",
        "# %%time\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "y_pred = nb.predict(X_test)\r\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\r\n",
        "print(classification_report(y_test, y_pred,target_names=labels))\r\n",
        "print(y_pred)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.8960526315789473\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.86      0.85      0.86      1900\n",
            "      Sports       0.86      0.88      0.87      1900\n",
            "    Business       0.95      0.97      0.96      1900\n",
            "    Sci/Tech       0.91      0.88      0.90      1900\n",
            "\n",
            "    accuracy                           0.90      7600\n",
            "   macro avg       0.90      0.90      0.90      7600\n",
            "weighted avg       0.90      0.90      0.90      7600\n",
            "\n",
            "['Business' 'Sci/Tech' 'Sci/Tech' ... 'Sports' 'Business' 'Business']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lI-kgnP7MI7",
        "outputId": "f1834793-ef8d-4e63-a149-31ac27b3e073"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\r\n",
        "#Creating NLP pipeline model using Linear Support Machine Algorithm\r\n",
        "sgd = Pipeline([('vect', vectorizer),\r\n",
        "                ('tfidf', TfidfTransformer()),\r\n",
        "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\r\n",
        "               ])\r\n",
        "sgd.fit(X_train, y_train)\r\n",
        "\r\n",
        "# %%time\r\n",
        "\r\n",
        "y_pred = sgd.predict(X_test)\r\n",
        "\r\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\r\n",
        "print(classification_report(y_test, y_pred,target_names=labels))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.8793421052631579\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.86      0.82      0.84      1900\n",
            "      Sports       0.87      0.83      0.85      1900\n",
            "    Business       0.89      0.98      0.93      1900\n",
            "    Sci/Tech       0.90      0.88      0.89      1900\n",
            "\n",
            "    accuracy                           0.88      7600\n",
            "   macro avg       0.88      0.88      0.88      7600\n",
            "weighted avg       0.88      0.88      0.88      7600\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlO78FsC9fnF",
        "outputId": "1780503b-b8c8-4a49-e77c-8797955f855b"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "#Creating NLP pipeline model using Logistic Regression Algorithm\r\n",
        "logreg = Pipeline([('vect', vectorizer),\r\n",
        "                ('tfidf', TfidfTransformer()),\r\n",
        "                ('clf', LogisticRegression(n_jobs=1, C=1e5 , max_iter=600 , solver=\"saga\")),\r\n",
        "               ])\r\n",
        "logreg.fit(X_train, y_train)\r\n",
        "\r\n",
        "# %%time\r\n",
        "\r\n",
        "y_pred = logreg.predict(X_test)\r\n",
        "\r\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\r\n",
        "print(classification_report(y_test, y_pred,target_names=labels))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.8664473684210526\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.82      0.81      0.81      1900\n",
            "      Sports       0.82      0.85      0.84      1900\n",
            "    Business       0.94      0.94      0.94      1900\n",
            "    Sci/Tech       0.88      0.86      0.87      1900\n",
            "\n",
            "    accuracy                           0.87      7600\n",
            "   macro avg       0.87      0.87      0.87      7600\n",
            "weighted avg       0.87      0.87      0.87      7600\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuNptlyK9meS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}